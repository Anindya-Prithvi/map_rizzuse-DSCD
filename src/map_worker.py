import os
import secrets
from concurrent import futures

import grpc
from loguru import logger

import messages_pb2
import messages_pb2_grpc


class MapProcessInput(messages_pb2_grpc.MapProcessInputServicer):
    def __init__(self, mapper):
        self.mapper = mapper

    def Receive(self, request, context):
        # print("received", request.key, request.value)
        if request.key == "<EOF>":
            self.mapper.ready_to_reduce = True
            # block everything until reducer finishes
            # reducer code here
            __import__(name="time").sleep(__import__(name="random").randint(5, 10))

        return messages_pb2.Success(value="SUCCESS")


class Mapper:
    def __init__(self, PORT, IP):
        # create directory to store intermediate files
        self.intermediate_dir = f"map_{secrets.token_urlsafe(8)}"
        self.ready_to_reduce = False
        self.mapped_files = []

        if not os.path.exists("../map_intermediate"):
            os.mkdir("../map_intermediate")

        os.mkdir("../map_intermediate/" + self.intermediate_dir)

        port = str(PORT)
        server = grpc.server(futures.ThreadPoolExecutor(max_workers=50))
        self.server = server
        messages_pb2_grpc.add_MapProcessInputServicer_to_server(
            MapProcessInput(self), server
        )
        server.add_insecure_port(IP + ":" + port)  # no TLS moment
        server.start()
        server.wait_for_termination()

    def map(self, key, value, input_files):
        """Map function to each input split to generate
        intermediate key-value pairs. The Map function takes a
        key-value pair as input and produces a set of
        intermediate key-value pairs as output. The output of
        each Map function should be written to a file in the
        mapper's directory on the local file system. Note
        that each mapper will be run as a different process.
        """
        for input_file in input_files:
            with open(os.path.join(self.input_data, input_file), "r") as f:
                # Read the input data file
                input_data = f.read()
                lines = input_data.split("\n")

                words = []
                for line in lines:
                    line_words = line.split()
                    words.extend(line_words)

                intermediate_key_values = set()
                # TODO: Check whether the frequency will be updated in here
                for word in words:
                    intermediate_key_values.add((word, "1"))

                # Write the intermediate key-value pairs to a file in the mapper's dir
                output_file = os.path.join(self.output_data, input_file)
                with open(output_file, "w") as f:
                    for intermediate_key, intermediate_value in intermediate_key_values:
                        f.write("{}\t{}\n".format(intermediate_key, intermediate_value))

    def partition(self):
        """write a function that takes the list of key-value
        pairs generated by the Map function and partitions them
        into a set of smaller partitions. The partitioning
        function should ensure that all key-value pairs
        with the same key are sent to the same partition.
        Each partition is then picked up by a specific reducer
        during shuffling and sorting.
        """
        pass
