"""This is the file for master node"""


from loguru import logger
import argparse
import multiprocessing
from map_worker import Mapper
from reduce_worker import Reducer


class Master:
    """Master node class"""

    def __init__(self, input_data, output_data, n_map, n_reduce):
        self.input_data = input_data
        self.output_data = output_data
        self.n_map = n_map
        self.n_reduce = n_reduce
        self.mappers = []
        self.reducers = []
        logger.debug("Master node initialized. Starting child nodes.")
        self.initialize_nodes()
        logger.debug("Initializing complete.")

    def run(self):
        logger.debug("Starting map phase.")
        self.map()
        logger.debug("Map phase complete. Starting partition phase.")
        self.partition()
        logger.debug("Partition phase complete. Starting reduce phase.")
        self.reduce()
        logger.debug("Reduce phase complete. Job complete.")

    def initialize_nodes(self):
        """Initialize and register the mappers"""
        for i in range(self.n_map):
            p = multiprocessing.Process(target=Mapper, args=(None))
            p.start()
            self.mappers.append(p)

        """Initialize and register the reducers"""
        for i in range(self.n_reduce):
            p = multiprocessing.Process(target=Reducer, args=(None))
            p.start()
            self.reducers.append(p)
        

    def input_split(self):
        """For simplicity, you may assume that the input data
        consists of multiple data files and each file is
        processed by a separate mapper.
        """
        pass

    def map(self):
        """Map function to each input split to generate
        intermediate key-value pairs. The Map function takes a
        key-value pair as input and produces a set of
        intermediate key-value pairs as output. The output of
        each Map function should be written to a file in the
        mapper's directory on the local file system. Note
        that each mapper will be run as a different process.
        """
        pass

    def partition(self):
        """write a function that takes the list of key-value
        pairs generated by the Map function and partitions them
        into a set of smaller partitions. The partitioning
        function should ensure that all key-value pairs
        with the same key are sent to the same partition.
        Each partition is then picked up by a specific reducer
        during shuffling and sorting.
        """
        pass


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--input", help="Input data directory", required=True)
    parser.add_argument("--output", help="Output data directory", required=True)
    parser.add_argument("--n_map", help="Number of mappers", required=True)
    parser.add_argument("--n_reduce", help="Number of reducers", required=True)
    args = parser.parse_args()

    master = Master(args.input, args.output, args.n_map, args.n_reduce)
    master.run()